{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d8c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "import seaborn as sns\n",
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "import utils.lstmModel as lstm\n",
    "from utils.logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b74ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pontos no bbox do Rio: 21\n",
      "   latitude  longitude\n",
      "0    -23.05     -43.75\n",
      "1    -23.05     -43.65\n",
      "2    -23.05     -43.55\n",
      "3    -23.05     -43.45\n",
      "4    -23.05     -43.35\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/pbose/dataset/merge/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccess_merge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccess_merge\u001b[39;00m\n\u001b[1;32m      4\u001b[0m access_merge\u001b[38;5;241m.\u001b[39mlistar_pontos_merge()\n\u001b[0;32m----> 5\u001b[0m timeseries \u001b[38;5;241m=\u001b[39m \u001b[43maccess_merge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macessar_dados_merge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m timeseries[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchuva\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog1p(timeseries[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchuva\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m num_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m\n",
      "File \u001b[0;32m~/tcc/pesquisa/mergeDataset/access_merge.py:11\u001b[0m, in \u001b[0;36macessar_dados_merge\u001b[0;34m(caminho_base, logger)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21macessar_dados_merge\u001b[39m(caminho_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/pbose/dataset/merge/\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Abre o arquivo GRIB2 como lista de datasets\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     arquivos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([\n\u001b[1;32m     10\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(caminho_base, f)\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaminho_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.grib2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMERGE_CPTEC_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     ])\n\u001b[1;32m     14\u001b[0m     latRio_min, latRio_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m23.05\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m22.75\u001b[39m\n\u001b[1;32m     15\u001b[0m     lonRio_min, lonRio_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m43.79\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m43.10\u001b[39m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/pbose/dataset/merge/'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import access_merge as access_merge\n",
    "access_merge.acessar_dados_merge_lat_long()\n",
    "timeseries = access_merge.acessar_dados_merge()\n",
    "timeseries['chuva'] = np.log1p(timeseries['chuva'])\n",
    "\n",
    "num_features = 18\n",
    "# 2. FEATURES TEMPORAIS\n",
    "timeseries['dia_seno'] = np.sin(2 * np.pi * timeseries.index.dayofyear / 365)\n",
    "timeseries['dia_cosseno'] = np.cos(2 * np.pi * timeseries.index.dayofyear / 365)\n",
    "timeseries['mes_seno'] = np.sin(2 * np.pi * timeseries.index.month / 12)\n",
    "timeseries['mes_cosseno'] = np.cos(2 * np.pi * timeseries.index.month / 12)\n",
    "timeseries['ano'] = timeseries.index.year - timeseries.index.year.min()\n",
    "\n",
    "\n",
    "timeseries['chuva_ma3']  = timeseries['chuva'].shift(1).rolling(window=3, min_periods=1).mean().fillna(0)\n",
    "timeseries['chuva_ma7']  = timeseries['chuva'].shift(1).rolling(window=7, min_periods=1).mean().fillna(0)\n",
    "timeseries['chuva_ma14'] = timeseries['chuva'].shift(1).rolling(window=14, min_periods=1).mean().fillna(0)\n",
    "timeseries['chuva_ma30'] = timeseries['chuva'].shift(1).rolling(window=30, min_periods=1).mean().fillna(0)\n",
    "\n",
    "# 2. Estatísticas móveis\n",
    "timeseries['chuva_std7'] = timeseries['chuva'].shift(1).rolling(window=7, min_periods=1).std().fillna(0)\n",
    "timeseries['chuva_max7'] = timeseries['chuva'].shift(1).rolling(window=7, min_periods=1).max().fillna(0)\n",
    "timeseries['chuva_min7'] = timeseries['chuva'].shift(1).rolling(window=7, min_periods=1).min().fillna(0)\n",
    "\n",
    "# 3. Lags\n",
    "timeseries['chuva_lag1'] = timeseries['chuva'].shift(1).fillna(0)\n",
    "timeseries['chuva_lag3'] = timeseries['chuva'].shift(3).fillna(0)\n",
    "timeseries['chuva_lag7'] = timeseries['chuva'].shift(7).fillna(0)\n",
    "\n",
    "# 5. Flags binários\n",
    "timeseries['choveu_ontem'] = (timeseries['chuva_lag1'] > 0).astype(int)\n",
    "timeseries['choveu_semana'] = (timeseries['chuva_ma7'] > 0).astype(int)\n",
    "\n",
    "features_dinamicas = [col for col in timeseries.columns if 'chuva' in col]\n",
    "features_sazonais = ['dia_seno', 'dia_cosseno', 'mes_seno', 'mes_cosseno', 'ano']\n",
    "scaler_chuva = MinMaxScaler()\n",
    "timeseries[features_dinamicas] = scaler_chuva.fit_transform(timeseries[features_dinamicas])\n",
    "\n",
    "logger.info(f\"Dados carregados com sucesso. Total de {len(timeseries)} registros.\")\n",
    "datas = timeseries.index \n",
    "\n",
    "plt.plot(timeseries['chuva'])\n",
    "plt.title('Daily Rain in station')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('mm')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "corr = timeseries.corr(numeric_only=True)\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlação entre as variáveis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 14\n",
    "logger.info(f\"Preparando sequências com um lookback de {lookback} dias.\")\n",
    "X, y = lstm.create_sequence(timeseries.values, lookback)\n",
    "dates_aligned = datas[lookback:]\n",
    "\n",
    "train_size = int(len(X) * 0.70)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "train_date, test_date = dates_aligned[:train_size] , dates_aligned[train_size:]\n",
    "logger.info(f\"Sequências criadas. Treino: {len(X_train)} amostras, Teste: {len(X_test)} amostras.\")\n",
    "logger.info(f\"Shape {X_train.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91412689",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = KerasRegressor(model=lstm.criar_modelo_avancado, verbose=0)\n",
    "n_splits = 3 # Número de divisões para a validação cruzada\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Cria a instância do GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=model_wrapper,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,          # Validação cruzada para séries temporais\n",
    "    n_jobs=-1,        # Usar todos os processadores disponíveis\n",
    "    verbose=2         # Mostra o progresso\n",
    ")\n",
    "\n",
    "# Inicia a busca (isso pode demorar MUITO)\n",
    "grid_result = grid.fit(X_train, y_train) \n",
    "\n",
    "logger.info(\"GridSearchCV finalizado.\")\n",
    "logger.info(f\"Melhores parâmetros encontrados: {grid_result.best_params_}\")\n",
    "logger.info(f\"Melhor score de validação cruzada (negativo da MSE): {grid_result.best_score_:.4f}\")\n",
    "\n",
    "# Você pode pegar o melhor modelo treinado para fazer previsões\n",
    "best_model = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681842b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 32\n",
    "# Building the model\n",
    "\n",
    "model = lstm.criar_modelo_avancado(\n",
    "    lookback=lookback,\n",
    "    n_features=num_features,\n",
    "    units_camada1=128,\n",
    "    units_camada2=64,\n",
    "    dropout_rate=0.2\n",
    ")\n",
    "logger.info(f\"Iniciando treinamento por {epochs} épocas. Com batch_size: {batch_size}\")\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "# patience=15: espera 30 épocas sem melhora antes de parar\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=25, min_lr=1e-6, verbose=1)\n",
    "# factor=0.2: reduz o learning rate para 20% do valor atual\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    "    #callbacks=[ early_stopping, reduce_lr ]\n",
    ")\n",
    "logger.info(\"Treinamento concluído.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee774d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Previsão ---\n",
    "logger.info(\"Realizando previsões no conjunto de teste.\")\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "n_features_chuva = scaler_chuva.n_features_in_\n",
    "pred_dummy = np.zeros((len(pred), n_features_chuva))\n",
    "pred_dummy[:, 0] = pred.flatten()\n",
    "pred_log = scaler_chuva.inverse_transform(pred_dummy)[:, 0]\n",
    "pred = np.expm1(pred_log)\n",
    "\n",
    "y_test_dummy = np.zeros((len(y_test), n_features_chuva))\n",
    "y_test_dummy[:, 0] = y_test.flatten()\n",
    "y_testlog = scaler_chuva.inverse_transform(y_test_dummy )[:, 0]\n",
    "y_test = np.expm1(y_testlog)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "mse = mean_squared_error(y_test, pred)\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "logger.info(\"--- Métricas de Avaliação no Conjunto de Teste ---\")\n",
    "logger.info(f\"RMSE (Raiz do Erro Quadrático Médio): {rmse:.4f}\")\n",
    "logger.info(f\"MSE (Erro Quadrático Médio): {mse:.4f}\")\n",
    "logger.info(f\"MAE (Erro Absoluto Médio): {mae:.4f}\")\n",
    "logger.info(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Gerando gráfico de previsão final.\")\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(test_date, y_test, label=\"Real\")\n",
    "plt.plot(test_date, pred, label=\"Previsto\")\n",
    "plt.legend()\n",
    "plt.title(\"Previsão de Chuva com BiLSTM\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Chuva\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "logger.info(\"Script BiLSTM BrDwgd(TensorFlow/Keras) finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
